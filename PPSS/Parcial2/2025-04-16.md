
En la sesión anterior probamos las propiedades emergentes funcionales. En está probaremos las no funcionales con JMeter. 

No tienen sentido hacer un tipo de prueba sin haber hecho los otros tipos de pruebas anteriores. 

En el nivel de aceptación mi objetivo no es encontrar defectos.
Quiero comprobar en que grado el sistema satisface los criterios de aceptación. Que satisface lo que quiere el cliente. 

En el nivel de aceptación dividimos en dos partes:
- Unas que serán automáticas 
- Unas donde el propio usuario probará
	- Como usuario va a ser mucho más difícil de depurar porque el problema lo he elegido yo como si fuera el usuario

Nos basamos en los criterios de aceptación para diseñar las pruebas. Con propiedades emergentes:
- Funcionales 
	- Selenium WebDriver
- No funcionales
	- No las puedo automatizar con WebDriver
	- Utilizaremos JMeter
		- No es un lenguaje nuevo pero es otra forma de programar

## Propiedades emergentes no funcionales


Las propiedades no funcionales suelen terminar en "-ilidades":
- fiabilidad
	- la probabilidad de funcionar sin fallos durante un tiempo determinado en un entorno específico
		- Como es una probabilidad lo solemos representar en porcentaje
		- ejemplo, 90%
- disponibilidad
	- El tiempo durante el cual el sistema está disponible, activa
		- Se suele representar horas/días (24/7)
- Mantenibilidad - la capacidad de un sistema para soportar cambios. Tres tipos de cambios:
	- correctivo 
		- provocados por errores detectados en la app
	- adaptativo 
		- provocados por cambios de HW o SW
	- perfectivo
		- se quiere añadir o modificar una funcionalidad existente amplia y mejor el negocio
- escalabilidad
	- la capacidad de mantener el tiempo de respuesta ante cambios en el número de usuarios que utilizan el sistema 
O no
- Robustez
	- capacidad de un sistema para seguir funcionando a pesar de los fallos 


## Métricas

Ahora la pregunta no es "el cliente está contento o no?" es el grado de satisfacción. Usamos un número, cuantificamos cada una de las propiedades. Sino no podemos hacer nada con ellas. Necesito usar una formula. 

Para cada criterio de aceptación se utiliza una métrica diferente:
- para fiabilidad 
	- usamos pruebas aleatorias basándonos en un perfil operacional
		- MTTF 
			- `Mean Time To Failure`
		- MTTR 
			- `Mean Time to repair`
		- MTBF $$ MTBF = MTTF + MTTR$$
			- `Mean time beetween failures`
- para disponibilidad 
	- se usa MTTR para medir el `downtime` y la idea es minimizar este valor 
- para Mantenibilidad 
	- se usa la métrica MTTR 
		- que refleja el tiempo consumido en analizar un defecto correctivo, diseñar la modificación, implementarla, probarlo y distribuirlo 
- para la escalabilidad se utiliza el n.º de transacciones (operaciones) por unidad de tiempo
	- los sistemas pueden incrementar su escalabilidad siempre y cuando no sobrepasen la limitaciones de 
		- almacenamiento 
		- ancho de banda 
		- velocidad de procesador


## Como selecciono casos de prueba

- Prueba de carga 
	- Valida el rendimiento de un sistema 
	- cuanto soy capaz de producir en poco tiempo 
	- tengo que medirlo de alguna manera 
		- Por ejemplo, una petición de sistema se debe tratar en menos de 2 segundo cuando existen 1000 usuarios 
			- Eso si lo puedo medir
- Las pruebas de stress
	- Habrá picos de solicitudes y queremos ver que el sistema aguanta. Tenemos que ver cuanto tiempo aguanta. 
	- Para ello forzamos las peticiones por encima del límite del diseño 
		- se ha diseñado para aguantar 300 transacciones por segundo. Comenzamos con una carga inferior a 300 y vamos incrementarlo hasta ver cuando falla
	- estas pruebas comprueban fiabilidad y robustez 
		- cuando supera la carga normal - robustez es la capacidad de recuperación del sistema ante entradas erróneas u otros fallos.
- Pruebas estadísticas 
	- para evaluar la fiabilidad de un sistema podemos utilizar estás pruebas 
		- construir un perfil operacional
			- que refleje el uso real del sistema (patrón de entradas)
		- como resultado se identifican las clases de entrada y la posibilidad de ocurrencia de cada clase
			- asumiendo el uso normal diseñado en los casos de prueba
		- generar un conjuntos de dados de prueba 
		- probar los datos midiendo el n.º de fallos y tiempos entre fallos
			- calcula la fiabilidad después de observar un n.º de fallos significativo

## Pruebas estadísticas

El perfil operacional es la base para el diseño de pruebas emergentes no funcionales

En una aplicación cuando la usamos no ejecutamos todas la funcionalidades una a una. No, solo ejecutamos un par cada vez que entramos. Por lo que, vemos que hay funcionalidades que se ejecutan con más frecuencia

![[Pasted image 20250416114300.png]]

Si no puedo estimar esto. Mis casos de prueba tienen que tener más pruebas que en las más usadas. Donde van a saltar más errores? En las funcionalidades más usadas. 

La gráfica no se ajusta a la tabla, es más representativo que otra cosa. Si estuviera bien hecho se ajustaría.

Se generarían n.º aleatorios entre 1 y 99. Se derivan casos de prueba según su distribución de probabilidad. 
Después deberíamos ejecutar las pruebas midiendo el n.º de fallos y el tiempo entre fallos

## Resumen del proceso de pruebas no funcionales

Las propiedades emergentes influyen en el rendimiento (performance) del sistema

Para evaluar el rendimiento:
1. Identificar los criterios de aceptación 
2. Diseñar / seleccionar
	1. Casos de pruebas de funcionalidades, escenarios reales
	2. no seleccionamos datos de entrada
3. Preparar
	1. Preparar el entorno de pruebas
	2. Yo no puedo hacer las pruebas en la maquina de empresa que va como una moto. Tengo que preparar el entorno para algo que el usuario final va a utilizar
4. Automatizar
	1. Tengo que ejecutar las pruebas
	2. no JUNIT
		1. Cuando ejecutaba Junit obtenía la respuesta inmediata, sabía que cuantos fallan
	3. Grabaremos los escenarios de prueba y ejecutaremos los test
	4. Aquí no, aquí obtengo un montón de dados y después
5. Analizamos esos resultados que hemos obtenido de la automatización, que tenemos que hacer nosotros y la herramienta no nos va a ayudar con eso 

No se puede dejar las pruebas de rendimiento para el final 

Posibles fallos que pueden llegar a retocar mucho código:
- Las propiedades emergentes no funcionales no tienen, si o si, que estar condicionadas por la arquitectura del sistema
- La arquitectura se define en la fase de desarrollo. Si intentas cambiarlo a esas alturas supondría cambiarlo todo. La idea es definirla con la idea de rendimiento. 
	- con metodología de desarrollo iterativa 
- Lo que se tarda en reparar un error siempre es proporcional al tiempo que ha pasado desde que se implementó 



## JMeter

JMeter es una herramienta de escritorio java diseñada para medir el rendimiento mediante pruebas de carga.
Permite múltiples hilos de ejecución concurrentes y procesa varios patrones de petición a la vez
- JMeter trabaja con muchos tipos de aplicaciones 
	- a cada una de ellas le proporciona un `Sampler o muestreador` para hacer peticiones

No llama driver a los test, los llama testPlan. 
El "Código" consiste en una serie de elementos que pueden anidarse formando un "Árbol". 
Cada tipo de elemento tiene su icono..

`TestPlan` o plan de pruebas
---
describe una serie de "pasos" o acciones que JMeter realiza cuando se ejecuta el plan 

Está formado por:
- Uno o más grupos de hilos (`Thread Group`)
- Controladores lógicos (`Logic Controller`)
- `Samplers, Listeners, Timers, Assertions, Pre-Processors, Post-Processors`
- Elementos de configuración (`Configuration Elements`)

No nos centraremos en los `processors`

El orden de ejecución de los elementos de un plan es 
1. elemento de configuración 
2.  `Pre-Processors`
3. `Timers`
4. `Sampler` que puede verse afectado por varios `timers y elementos de configuración, asserciones...`
5. ` Post-Processors
6. `Assertions si el resultado del sampler no es null`
7. `Listener si el resultado del sampler no es null`

Hilos de ejecución `Thread Group` 
---

Cada hilo (representa un usuario) ejecuta el plan de forma independiente a otros hilos 

Podemos ejecutar el grupo de hilos un cierto número de veces o de forma indefinida ("infinito")

Todos los controladores y samplers deben estar anidados a un `Thread Group` 

`Ramp-up` (periodo de subida)
- sirve para que los hilos se creen de forma gradual. Esto permite comprobar cómo rinde el servidor conforme crece la carga
	- por ejemplo si el periodo de subida es de 100 segundos y el número de hilos es de 50. Tardará 100 segundos en crear 50 hilos, es decir, un nuevo hilo por cada 2 segundos

`Samplers o muestradores `
---
envía peticiones a un servidor 
- `HTTP, FTP request, HDBC request,` 
se ejecuta en el orden que aparece en el plan 
podemos configurar alguna propiedad añadiendo uno o varios `Configuration elements`

En la práctica nosotros usamos `HTTP request`

No todos los elementos pueden anidarse dentro de otros, Un `Sample` puede anidarse en un grupo de hilos o/y en un controlador 

Su ejecución puede verse afectada por otros elementos 
y puede anidar los elementos ` Listeners, Timers, Assertions, Pre-Processors, Post-Processors, elemento de configuración` estos elementos anidados solo afectarán a este Sampler 

```
	Test Plan
		|
		- Thread Group
			- One 
				- Assertion 1
			- Simple controller 
				- two
				- three
				- Assertion 2
			- Four

La Assertion 2 afecta tanto a two como a three
One, Two y Three y Four son samples
```

Controlador Lógico
----
Determina en que orden se procesan los Samplers. 
Actúa sobre sus hijos 

Ejemplos de controladores 
```
- Simple Controller
	- No tiene efecto sobre sus hijos, sirve para agruparlos

- Loop Controller
	- itera sobre sus hijos un cierto n.º de veces

- Once only Controller 
	- Indica a jmeter que sus hijos deben ser procesador una única vez por hilo 
	- si este controlador es hijo de un loop controller 
		- solo se ejecutará en la primera iteración de ese bucle

- Interleave controller 
	- ejecutará uno de los subcontroladores o samplers en cada iteración del bucle de pruebas a lo largo de la lista
```

Ejemplo 1
```
Test Plan
- Thread Group de 2 iteraciones
	- Once Only Controller 
		- Login Request (HTTP Sampler) // solo se ejecuta una vez (si el hilo tiene varias iteraciones se ejecutará en la primera)
	- Load Search Page (HTTP Sampler) //Accedemos a la página de busqueda
	- Interleave Controller 
		- Search A (HTTP Sampler)
		- Search B (HTTP Sampler)
		- Http default request (conf elem)
	- http default request (conf elem) //afecta a todos
	- cookie manager (conf elem)
```

siendo el `Interleave Controller` seria equivalente a 
```
	Load Search Page (HTTP Sampler)
	Search A (HTTP Sampler)
	Load Search Page (HTTP Sampler)
	Search B (HTTP Sampler)
```

Ejemplo 2

Tenemos un grupo de hilos formados por 2 hilos y 5 iteraciones:

```
Test Plan 
- Jakarta Users
	- Interleave Controller 
		- News Page
		- FAQ Page
		- Gump Page
	- Log Page 
	- File Reporter 
```



| `Loop Iteration` | `Each JMeter Thread Sends These HTTP Request`                                                             |
| ---------------- | --------------------------------------------------------------------------------------------------------- |
| 1                | `New Page`                                                                                                |
| 1                | `Log Page`                                                                                                |
| 2                | `Faq page`                                                                                                |
| 2                | `Log Page`                                                                                                |
| 3                | `Gump Page`                                                                                               |
| 3                | `Log page`                                                                                                |
| 4                | `Como no hay muchas peticiones en el controler Jmeter vuelve a enviar la primera http request : New Page` |
| 4                | `Log page`                                                                                                |
| 5                | `Faq page`                                                                                                |
| 5                | `Log page `                                                                                               |

Cada hilo realiza 10 peticiones (porque son 5 iteraciones y dos hilos )

El controlador de intervalo hace que ejecute cada iteración uno de sus hijos. Mientras que el `Log Page` se ejecuta siempre, independientemente de la iteración 

`El listener File Reporter` recopila los resultados de todos los elementos del grupo de hilos. 


Elementos de configuración
---

las peticiones `Samplers` pueden ser modificadas 
Un elemento de configuración es accesible solo dentro de la rama del árbol en la que se sitúa 

- `HTTP Request Defaults`
	- se usan cuando varios `Samplers` comparten la misma URL o parte de ella, así evitamos tener que especificarla en todas ellas
- `HTTP Cookie Manager`
	- Permite almacenar y enviar cookies como si fuese un navegador
```
	Test Plan
		|
		- Thread Group
			- Simple controller
				- Http cookie manager // afecta a WebPage 1 y 2
				- WebPage 1
				- Loop controller 
					- WebPage 2
					- Web Defaults 1 // solo afecta a WebPage 2
			- WebPage 3 
			- Web Default 2 //afecta a los 3 webPage

```

`Timers` o temporizadores
---
JMeter por defecto envía peticiones sin realizar ninguna pausa entre las mismas. Esto podría saturar el servidor al enviar demasiadas peticiones en intervalos de tiempo cortos

Los temporizadores permiten introducir pausas antes de realizar cada una de las peticiones de cada hilo 

Siempre se ejecutan antes que los `Samplers`

Existen varios tipos:
- `Constant timer`
	- retrasa cada petición de usuario la misma cantidad de tiempo 
- `Uniform random timer`
	- introduce pausas aleatorias
		- con la misma probabilidad
- `Gaussian random timer` 
	- Introduce pausas según una determinada distribución 

si hay varios temporizadores en el mismo nivel de `sampler` se aplicaran todos ellos

si queremos que solo afecte a un solo `Sample` lo añadimos como hijo 

Si queremos que se aplique después de un `sample` lo añadimos
- al siguiente `sampler`
- como hijo de un `Flow Control Action Sampler`


```
	Test Plan
		|
		- Thread Group
			- One 
			- Simple controller 
				- two
				- Timer 1 // se aplica a two, three y four
				- three
					- Assertion 1
				- Simple controller
					- Four
			- five
			- timer 2 // afecta a todos los samplers

```

```
	Test Plan
		|
		- Thread Group
			- http request default
			- http cookies manager
			- http header manager
			- home page
				- Assertion 
			- ThinkTimer1S // Flow control action sampler
				- URT // timer
			- Page Returning 404
				- assertion404
			- view Results tree

```

Aserciones 
--

Permiten afirmar sobre una respuesta recibida del servidor. Podemos añadir aserciones en cualquier `Sampler`. comprobamos si devuelve la respuesta esperada

Cualquier driver tiene que tener aserciones


`Listeners` 
----

se utilizan para ver y/o almacenar en el disco el resultado de las peticiones realizadas. Proporciona info que JMeter va acumulando sobre los casos de prueba a medida que se ejecutan los tests

- Todos los `Listeners guardan los mismos datos y se representan de manera de forma única`
- Ejemplos de `Listeners`
```
- Simple Data Writer
	- escribe los resultados en un formato sencillo a menudo a CSV o XML 
- Aggregate Graph 
	- genera gráficos de barras para visualizar los resultados de la prueba.
- Graph results
	- muestra los resultados en forma de gráfico. 
		- Es una representación de los datos de la prueba de rendimiento 
		- esto ayuda a analizar métricas como
			- tiempo de respuesta
			- rendimiento 
			- tasas de error
- Aggregate Report
	- También muestra los resultados en forma de tabla con cada sample que tarda en cada momento
- Summary Report
	- muestra un informe y lo muestra en forma de tabla
- Response Time Graph 
	- hace una grafica con los tiempos de respuesta
- Assertion Result
	- si alguna assercion falla lo muestra
	- sino muestra una lista de todos los ejecutados
- View result Tree
	- muestra una lista de todos los samples en cada momento que se han ejecutado en orden y puedes pulsar cada uno y ver los detalles
- View Results in Table
	- lo mismo que el tree pero con una tabla con tiempos y latencia 
```

El `Throughput` representa la capacidad del servidor para soportar una determinada carga. A mayor sea el valor, mayor será el rendimiento del servidor 
$$Throughput = rendimiento$$
$$Rendimiento = numPeticiones / SegundosTotales$$

JMeter calcula (en milisegundos si es tiempo)
```
- #Sampler
	- N.º de muestras con la misma etiqueta
- Average 
	- tiempo medio de respuesta
- median
	- tiempo en el medio del conjunto de resultados
- linea de 90%
	- 90% de las muestras no toma mas que ese tiempo 
- min
	- tiempo minimo de respuesta de la muestra
- max
	- tiempo maximo de respuesta de las muestras
- %error
	- porcentaje de errores de peticiones 
- Rendimiento (Throughput)
	- peticiones por segundo 
	- se mide desde la primera peticion hasta que recibe la ultima (Timers incluidos)
- Kb/sec
	- rendimiento en Kbytes por segundo 
```


Para hacer estos casos de prueba se necesita escenarios significativos del mundo real

Probar JMeter en distintas máquinas para que no influyan sobre los resultados

Los procesos de prueba son procesos científicos. Deben realizarse bajo condiciones controladas 
- Si estas trabajando en un servidor compartido
	- comprueba que nadie está haciendo pruebas de carga también 

Asegurar que se dispone de ancho de banda que ejecuta JMeter 
- La idea es probar el rendimiento de la aplicaciones y del servidor
- no de la conexión de red

Utiliza varias instancias de JMeter para ejecutarse en diferentes máquinas añadiendo carga adicional 

Deja que una prueba se ejecute durante un largo periodo de tiempo 
- Días / Semanas
